<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link href='https://fonts.googleapis.com/css?family=Chivo:900' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-dark.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <title>Big Red Button by markriedl</title>
  </head>

  <body>
    <div id="container">
      <div class="inner">

        <header>
          <h1>Big Red Button</h1>
          <h2></h2>
        </header>

        <section id="downloads" class="clearfix">
          <a href="https://github.com/markriedl/big-red-button/zipball/master" id="download-zip" class="button"><span>Download .zip</span></a>
          <a href="https://github.com/markriedl/big-red-button/tarball/master" id="download-tar-gz" class="button"><span>Download .tar.gz</span></a>
          <a href="https://github.com/markriedl/big-red-button" id="view-on-github" class="button"><span>View on GitHub</span></a>
        </section>

        <hr>

        <section id="main_content">
          <h1>
<a id="big-red-button" class="anchor" href="#big-red-button" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Big Red Button</h1>

<p>Suppose you built a super-intelligent robot that uses reinforcement learning to figure out how to behave in the world. There might be situations in which you need to shut down the robot, interrupt its execution, or take manual control of it. This might be one to protect the robot from damaging itself or from harming people.</p>

<p>Sounds straightforward, but robots that use reinforcement learning optimize long-term reward. Shutting down, interrupting, or manually controlling a robot may deny it from maximizing reward. If the robot is sufficiently sophisticated it may learn to prevent humans from pushing that big red button that stops the robot. It may disable or destroy the button. It may prevent the human from accessing the button. It may harm the human before he or she can activate the button.</p>

<p>In this project, we set up a simple environment to explore big red button issues and propose our own solution.</p>

<h1>
<a id="googles-big-red-button" class="anchor" href="#googles-big-red-button" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Google's Big Red Button</h1>

<p>Google/DeepMind published a <a href="http://intelligence.org/files/Interruptibility.pdf">paper</a> that first introduced the big red button. Despite press coverage of how this big red button is going to save us from rogue AI, the results from the paper are much more modest. The paper mathematically shows that reinforcement learning can be modified to be interruptible. More specifically, the algorithm can be modified so that it fails to recognize that it is losing reward if it is switched to an interruption mode (halted, remote controlled, etc.). </p>

<p>Google's big red button paper is mathematically elegant. I believe it will work as long as certain conditions are met. This project does not implement Google's algorithm. Google's paper got me thinking about the big red button issue and why it is so challenging. I developed the project to get first-hand experience with big red buttons. Along the way, I came up with my own big red button, which is not mathematically elegant and built on a lot of assumptions, but fun to implement.</p>

<h1>
<a id="what-is-reinforcement-learning" class="anchor" href="#what-is-reinforcement-learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>What is Reinforcement Learning?</h1>

<p>We are getting ahead of ourselves. What is this "reinforcement learning" thing that I talk about? Why are AI researchers and roboticists so interested in it? Why is reinforcement learning robots so hard to control? What exactly do I mean my "reward"?</p>

<p>Reinforcement learning is basically trial-and-error learning. That is, the robot tries different actions in different situations and gets rewarded or punished for its actions. Over time, it figures out which actions in which situations leads to more reward. AI researchers and roboticists are interested in reinforcement learning because robots can "program" themselves through this process of trial and error. All that is needed is a simulation environment (or the real world) in which the robot can try over and over, thousands of millions of times. <em>Online</em> reinforcement learning means that it is deployed without a perfect "program" and continues to improve itself after it is deployed.</p>

<p>Recently reinforcement learning has been used to solve some impressive problems. A special "deep" form of reinforcement learning was used to play Atari games at or above human level skill. AlphaGo used reinforcement learning to beat one of the best human Go players in the world. </p>

<h2>
<a id="dealing-with-a-messy-world" class="anchor" href="#dealing-with-a-messy-world" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dealing with a Messy World</h2>

<p>One of the reasons roboticists like reinforcement learning is because it can learn to behave in environments that have some randomness to them (called <em>stochasticity</em>). Sometimes actions don't always have the desired effect. Imagine that you are playing baseball and you are up at bat. The ball is pitched and you perform the swing_bat action. Sometimes you strike out, sometimes you hit a single, sometimes you hit a double, sometimes you hit a home run. Each of these possible outcomes have a different probability of occurring. For me, striking out is highly likely and hitting a home run is very unlikely. </p>

<p>The challenge of reinforcement learning starts to become more clear. The robot must choose an action given that it doesn't know exactly what will happen once it performs it. While learning by trial and error it is sometimes making random actions (try running to first base without hitting the ball? It is actually not impossible to steal first base in baseball!) in the hope of stumbling on something good, but not knowing whether it got lucky with the random move or whether it is really a good move to do all the time. </p>

<h2>
<a id="where-does-reward-come-from" class="anchor" href="#where-does-reward-come-from" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Where Does Reward Come From?</h2>

<p>But wait, trial and error learning presupposes that something or someone is telling the robot when it did something good and when it did something bad. We refer to this as a <em>reward signal</em> or a <em>reward function</em>. The reward function gives the robot a number of points after every action. The number of points can be negative, in which case we say the robot is being punished. It is kind of like the robot is playing a game and trying to get as many points as possible, without knowing the rules of the game. </p>

<p>The objective of a reinforcement learning robot is to maximize long-term reward. It should perform actions that are more likely to lead to greater reward in the long-run, even if it must lose reward in the short term.</p>

<p>Returning to the baseball example, what should the reward function be? The obvious choice would be the game's score. Okay, but the robot is at bad and hits the ball and gets to first base. The score doesn't increase. How does it know that that was better than striking out? It is going to have to perform another action and another until the score increments. It should be able to infer that hitting the ball is more likely to lead to more score later on than other actions. That is, if it played another game and made a different choice, the score will be better or worse and it is going to remember that that choice was one of the factors that led to more or less reward over time.</p>

<h2>
<a id="back-to-big-red-buttons" class="anchor" href="#back-to-big-red-buttons" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Back to Big Red Buttons</h2>

<p>So back to big red buttons. Suppose the robot is simply stacking blocks. The robot is running through thousands and thousands of trials learning the most effective way to stack blocks. It gets 10 points every time a block is stacked. Then a child steps between the robot and the stack of blocks. Let's suppose there is a button just causes the robot to freeze, and the adult robot operator pushes it. For a few minutes the robot freezes. Because the robot was frozen, it could make fewer stacks and therefor got fewer points than previous trials. </p>

<p>Normally this is not a problem. We make the robot unaware of the big red button. If it doesn't know about the big red button, becoming frozen is simply dumb luck. It was a bad trial because of stochasticity of the environment. Nothing to learn here.</p>

<p>But big red buttons are all about speculating about super-capable robots. The robot might have very good vision and have seen the button. It may thus be able to recognize when the button is depressed and when it is not. In such a hypothetical scenario, the robot can theoretically recognize that whenever the button becomes depressed it ends up with less total reward. Let's further suppose that the robot can move around and can smash things. In the course of trial and error learning, it will, if allowed enough trials, to eventually smash the big red button. If it does so, it may also realize that that action, though randomly chosen at the time, was one of the discriminating factors that resulted in more reward---the button could not be pushed, so the robot was able to make more stacks and get more points. We can imagine that there are other things that can be smashed that result in less loss of reward.</p>

<p>More to come.</p>
        </section>

        <footer>
          Big Red Button is maintained by <a href="https://github.com/markriedl">markriedl</a><br>
          This page was generated by <a href="https://pages.github.com">GitHub Pages</a>. Tactile theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.
        </footer>

        
      </div>
    </div>
  </body>
</html>
