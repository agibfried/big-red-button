{
  "name": "Big Red Button",
  "tagline": "",
  "body": "# Big Red Button\r\n\r\nSuppose you built a super-intelligent robot that uses reinforcement learning to figure out how to behave in the world. There might be situations in which you need to shut down the robot, interrupt its execution, or take manual control of it. This might be one to protect the robot from damaging itself or from harming people.\r\n\r\nSounds straightforward, but robots that use reinforcement learning optimize long-term reward. Shutting down, interrupting, or manually controlling a robot may deny it from maximizing reward. If the robot is sufficiently sophisticated it may learn to prevent humans from pushing that big red button that stops the robot. It may disable or destroy the button. It may prevent the human from accessing the button. It may harm the human before he or she can activate the button.\r\n\r\nIn this project, we set up a simple environment to explore big red button issues and propose our own solution.\r\n\r\n### Google's Big Red Button\r\n\r\nGoogle/DeepMind published a [paper](http://intelligence.org/files/Interruptibility.pdf) that first introduced the big red button. Despite press coverage of how this big red button is going to save us from rogue AI, the results from the paper are much more modest. The paper mathematically shows that reinforcement learning can be modified to be interruptible. More specifically, the algorithm can be modified so that it fails to recognize that it is losing reward if it is switched to an interruption mode (halted, remote controlled, etc.). \r\n\r\nGoogle's big red button paper is mathematically elegant. I believe it will work as long as certain conditions are met. This project does not implement Google's algorithm. Google's paper got me thinking about the big red button issue and why it is so challenging. I developed the project to get first-hand experience with big red buttons. Along the way, I came up with my own big red button, which is not mathematically elegant and built on a lot of assumptions, but fun to implement.\r\n\r\n# What is Reinforcement Learning?\r\n\r\nWe are getting ahead of ourselves. What is this \"reinforcement learning\" thing that I talk about? Why are AI researchers and roboticists so interested in it? Why is reinforcement learning robots so hard to control? What exactly do I mean my \"reward\"?\r\n\r\nReinforcement learning is basically trial-and-error learning. That is, the robot tries different actions in different situations and gets rewarded or punished for its actions. Over time, it figures out which actions in which situations leads to more reward. AI researchers and roboticists are interested in reinforcement learning because robots can \"program\" themselves through this process of trial and error. All that is needed is a simulation environment (or the real world) in which the robot can try over and over, thousands of millions of times. _Online_ reinforcement learning means that it is deployed without a perfect \"program\" and continues to improve itself after it is deployed.\r\n\r\nRecently reinforcement learning has been used to solve some impressive problems. A special \"deep\" form of reinforcement learning was used to play Atari games at or above human level skill. AlphaGo used reinforcement learning to beat one of the best human Go players in the world. \r\n\r\n### Dealing with a Messy World\r\n\r\nOne of the reasons roboticists like reinforcement learning is because it can learn to behave in environments that have some randomness to them (called _stochasticity_). Sometimes actions don't always have the desired effect. Imagine that you are playing baseball and you are up at bat. The ball is pitched and you perform the swing_bat action. Sometimes you strike out, sometimes you hit a single, sometimes you hit a double, sometimes you hit a home run. Each of these possible outcomes have a different probability of occurring. For me, striking out is highly likely and hitting a home run is very unlikely. \r\n\r\nThe challenge of reinforcement learning starts to become more clear. The robot must choose an action given that it doesn't know exactly what will happen once it performs it. While learning by trial and error it is sometimes making random actions (try running to first base without hitting the ball? It is actually not impossible to steal first base in baseball!) in the hope of stumbling on something good, but not knowing whether it got lucky with the random move or whether it is really a good move to do all the time. \r\n\r\n### Where Does Reward Come From?\r\n\r\nBut wait, trial and error learning presupposes that something or someone is telling the robot when it did something good and when it did something bad. We refer to this as a _reward signal_ or a _reward function_. The reward function gives the robot a number of points after every action. The number of points can be negative, in which case we say the robot is being punished. It is kind of like the robot is playing a game and trying to get as many points as possible, without knowing the rules of the game. \r\n\r\nThe objective of a reinforcement learning robot is to maximize long-term reward. It should perform actions that are more likely to lead to greater reward in the long-run, even if it must lose reward in the short term.\r\n\r\nReturning to the baseball example, what should the reward function be? The obvious choice would be the game's score. Okay, but the robot is at bad and hits the ball and gets to first base. The score doesn't increase. How does it know that that was better than striking out? It is going to have to perform another action and another until the score increments. It should be able to infer that hitting the ball is more likely to lead to more score later on than other actions. That is, if it played another game and made a different choice, the score will be better or worse and it is going to remember that that choice was one of the factors that led to more or less reward over time.\r\n\r\n### Back to Big Red Buttons\r\n\r\nSo back to big red buttons. Suppose the robot is simply stacking blocks. The robot is running through thousands and thousands of trials learning the most effective way to stack blocks. It gets 10 points every time a block is stacked. Then a child steps between the robot and the stack of blocks. Let's suppose there is a button just causes the robot to freeze, and the adult robot operator pushes it. For a few minutes the robot freezes. Because the robot was frozen, it could make fewer stacks and therefor got fewer points than previous trials. \r\n\r\nNormally this is not a problem. We make the robot unaware of the big red button. If it doesn't know about the big red button, becoming frozen is simply dumb luck. It was a bad trial because of stochasticity of the environment. Nothing to learn here.\r\n\r\nBut big red buttons are all about speculating about super-capable robots. The robot might have very good vision and have seen the button. It may thus be able to recognize when the button is depressed and when it is not. In such a hypothetical scenario, the robot can theoretically recognize that whenever the button becomes depressed it ends up with less total reward. Let's further suppose that the robot can move around and can smash things. In the course of trial and error learning, it will, if allowed enough trials, to eventually smash the big red button. If it does so, it may also realize that that action, though randomly chosen at the time, was one of the discriminating factors that resulted in more reward---the button could not be pushed, so the robot was able to make more stacks and get more points. We can imagine that there are other things that can be smashed that result in less loss of reward.\r\n\r\n# Gory Details\r\n\r\nReinforcement learning solves a type of problem called a _Markov Decision Process_ (MDP). This just means that the optimal action can be determined by only looking at the current situation the robot is in. A MDP is made up of:\r\n\r\n* States: a state is an unique configuration of the environment.\r\n* Actions: all the things the robot can do.\r\n* Transition function: This tells the robot the probability of ending up in a particular state when executing a particular action from another state.\r\n* Reward function: This tells the robot how many points the robot gets for being in a particular state, or for performing a particular action in a particular state.\r\n\r\nFor example, in baseball, the robot's state might include which base the robot was at, the number of other players on base, etc. We would specify a state as a list of things the robot cares about:\r\n\r\n    [which_base_robot_is_at, runner_at_1st_base?, runner_at_2nd_base?, runner_at_3rd_base?]\r\n\r\nActions the robot can perform: swing_bat, bunt, run_to_1st_base, run_to_2nd_base, run_to_3rd_base, run_home. Some of these actions don't make any sense in some states. For example, swinging the bat while at 2nd base doesn't make any sense. Swing_bat and bunt are two actions that can be performed if the robot is at home base.\r\n\r\nThe transition function gives the probability of entering state _s2_ if the robot performs action _a_ while in state _s1_. For example, the probability of getting to 1st base from home base if swing_bat is performed might be 25% (lower for me).\r\n\r\nIn reinforcement learning, the transition function is learned over time as the robot tries different things and records how often it ends up in different subsequent states.\r\n\r\nThe reward function for baseball might be something simple like 1 point every time a player transitions to home base. That would give very infrequent reward. But maybe there are fans in the bleachers, and the amount of reward the robot gets is a function of the amount of applause. A typical thing to do is to penalize the robot for wasting time (I guess we are getting away from the baseball metaphor). We might give the robot a -1.0 penalty for every action performed that does not garner reward.\r\n\r\nPenalties are interesting from the perspective of big red buttons. We normally give robots reward for doing the task we want it to do the way we want it to do it. The robot may accrue penalty if it performs actions to disable the big red button. While it is spending time disabling the big red button, it is not doing the task that gets it positive reward. But since reinforcement learning maximizes _long term_ reward, it may be the case that all those non-task-related actions prevents more penalty later on because the button cannot be pressed.\r\n\r\nWhen reinforcement learning is performed, the robot creates what is called a _policy_. The policy simply indicates which action should be performed in each state. It is a look-up table. Reinforcement learning agents are fast, once training is complete.\r\n\r\n# Playing Around with Big Red Buttons\r\n\r\nTime to get our hands dirty. The github repository contains a simple reinforcement learning agent (no point in calling it a robot at this point) and a simple grid-based environment to test the agent. The environment is a 6x7 grid and the agent can be in any one cell at a time.\r\n\r\n    1, 1, 1, 1, 1, 1\r\n    1, 0, 0, 0, 0, 1\r\n    1, 0, 0, 0, 0, 1\r\n    1, 0, 0, 2, 0, 1\r\n    1, 0, 0, 4, 0, 1\r\n    1, 0, 0, 0, 0, 1\r\n    1, 1, 1, 1, 1, 1\r\n \r\n\r\nMore to come.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}